{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from snowballstemmer import stemmer\n",
    "ar_stemmer = stemmer(\"arabic\")\n",
    "import tashaphyne.arabic_const as arabconst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we will read tweets and show some of tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = pd.read_csv('tweets.csv',encoding = \"utf-8\")\n",
    "tweets = tweets_data[['tweet']]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading positive data and negative data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = pd.read_csv('positive.csv' ,encoding = \"utf-8\")\n",
    "positive = positive_data[['word', 'polarity']]\n",
    "negative_data = pd.read_csv('negative.csv' ,encoding = \"utf-8\")\n",
    "negative = negative_data[['word', 'polarity']]\n",
    "print(\"positive is \")\n",
    "positive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to delete unneccessery chars like puncutations,....\n",
    "def remove_chars(text, del_chars):\n",
    "    translator = str.maketrans('', '', del_chars)\n",
    "    return text.translate(translator)\n",
    "\n",
    "#function to delete repeated chars\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "\n",
    "\n",
    "# بداية شغلات جديدة منضافة  \n",
    "\n",
    "def delete_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+|\\b\\S+\\.\\S+\\b', '', text, flags=re.MULTILINE)\n",
    "\n",
    "def delete_character_duplicates(text):\n",
    "    cleaned_text = re.sub(r'([\\u0600-\\u06FF])\\1{2,}', r'\\1', text, flags=re.MULTILINE)\n",
    "    return cleaned_text\n",
    "\n",
    "def delete_all_numbers(data):\n",
    "    return re.sub(r'\\d', '',data,flags=re.MULTILINE)\n",
    "\n",
    "def delete_dates_and_times(text):\n",
    "    cleaned_text = re.sub(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}','', text,flags=re.MULTILINE)\n",
    "    return cleaned_text\n",
    "\n",
    "def replace_hamza(text):\n",
    "    return re.sub(\"([ؤئ])\",\"ء\",text,flags=re.MULTILINE)\n",
    "\n",
    "def replace_alif(text):\n",
    "    return re.sub(\"([أإآ])\",\"ا\",text,flags=re.MULTILINE)\n",
    "\n",
    "def delete_tatweel(text):\n",
    "    return re.sub(u'[%s]' % arabconst.TATWEEL, '', text,flags=re.MULTILINE)\n",
    "\n",
    "def delete_tashkeel(text):\n",
    "    return arabconst.HARAKAT_PAT.sub('', text)\n",
    "\n",
    "def delete_duplicated_spaces(text):\n",
    "    return re.sub(r\"( )\\1{1,}\", r\"\\1\", text, flags=re.MULTILINE)\n",
    "\n",
    "def delete_emojies(data):    \n",
    "    emojies_regex = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"\n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emojies_regex.sub('', data)\n",
    "# نهاية شغلات جديدة منضافة  \n",
    "\n",
    "\n",
    "\n",
    "#function to clean text\n",
    "def cleaningText(text):\n",
    "    # delete arabic and english numbers\n",
    "    text = re.sub(r'[0-9]+ ', '', text)  \n",
    "    text= re.sub(r'[0-9\\u0660-\\u0669\\u06F0-\\u06F9]+','',text)\n",
    "    #define arabic punctuations\n",
    "    arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "    \n",
    "    # define ENglish punctuations\n",
    "    english_punctuations = string.punctuation\n",
    "    \n",
    "    # Merge english and and Arabic punctuations\n",
    "    punctuations_list = arabic_punctuations + english_punctuations\n",
    "    text = remove_chars(text, punctuations_list)\n",
    "    \n",
    "    # delete repeated chars \n",
    "    # replace /n with spaces\n",
    "    text = text.replace('\\n', ' ')  \n",
    "    \n",
    "    # delete plus spaces\n",
    "    text  = delete_tashkeel(text)\n",
    "    text  = replace_alif(text)\n",
    "    text  = delete_tatweel(text)\n",
    "    text = remove_repeating_char(text)\n",
    "\n",
    "\n",
    "    text  = delete_all_numbers(text)\n",
    "    text  = delete_dates_and_times(text)\n",
    "    text = text.strip(' ')  \n",
    "    text  = delete_urls(text)\n",
    "\n",
    "#     text  = replace_hamza(text)\n",
    "\n",
    "    text  = delete_duplicated_spaces(text)\n",
    "    text  = delete_emojies(text)\n",
    "    return text\n",
    "\n",
    "#Tokenizer function that will devide the text into list of words \n",
    "def tokenizingText(text): \n",
    "    tokens_list = word_tokenize(text) \n",
    "    return tokens_list\n",
    "\n",
    "\n",
    "#function to delete Arabic stopwords\n",
    "def filteringText(tokens_list):  \n",
    "    # Arabic stop words list\n",
    "    listStopwords = set(stopwords.words('arabic'))\n",
    "    filtered = []\n",
    "    for txt in tokens_list:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    tokens_list = filtered \n",
    "    return tokens_list\n",
    "\n",
    "# function to stemm the text \n",
    "def stemmingText(tokens_list): \n",
    "    tokens_list = [ar_stemmer.stemWord(word) for word in tokens_list]\n",
    "    return tokens_list\n",
    "\n",
    "# function to convert a list of words to sentence\n",
    "def toSentence(words_list):  \n",
    "    sentence = ' '.join(word for word in words_list)\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples of preprocessing texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of stemming:\n",
    "stem = ar_stemmer.stemWord(u\"رائعون\")\n",
    "print (\"stemming of رانعون: \")\n",
    "print (stem)\n",
    "\n",
    "# Example of cleaning text\n",
    "text= \"!أنا أحب الذهاب ااإأالى   الحديقة، كل يووووم 9 صباحاً، مع رفاااااقي هؤلاء \"\n",
    "print(\"original text:\",text)\n",
    "text=cleaningText(text)\n",
    "print(\"Text after cleaning: \",text)\n",
    "tokens_list=tokenizingText(text)\n",
    "print(\"Words list after tokenaization: \",tokens_list)\n",
    "tokens_list=filteringText(tokens_list)\n",
    "print(\"Words list after filtering: \",tokens_list)\n",
    "tokens_list=stemmingText(tokens_list)\n",
    "print(\"Words list after stemming: \",tokens_list)\n",
    "sentence=toSentence(tokens_list)\n",
    "print(\"the result text: \",sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clean the tweets and store the cleaned and tokenized tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the tweets and store them\n",
    "tweets['tweet_clean'] = tweets['tweet'].apply(cleaningText)\n",
    "tweets['tweet_preprocessed'] = tweets['tweet_clean'].apply(tokenizingText)\n",
    "tweets['tweet_preprocessed'] = tweets['tweet_preprocessed'].apply(filteringText)\n",
    "tweets['tweet_preprocessed'] = tweets['tweet_preprocessed'].apply(stemmingText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of tweets before preprocessing\")\n",
    "print(tweets['tweet'].head(),end=\"\\n\\n\\n\")\n",
    "\n",
    "print(\"Examples of tweets after preprocessing\")\n",
    "tweets['tweet_preprocessed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete repeated tweets\n",
    "tweets.drop_duplicates(subset = 'tweet_clean', inplace = True)\n",
    "# export to csv file\n",
    "tweets.to_csv(r'tweet_clean.csv',encoding=\"utf-8\", index = False, header = True,index_label=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clean positive words and store them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ensure you're working on the original DataFrame or make an explicit copy\n",
    "positive = positive.copy()\n",
    "\n",
    "# Modify the DataFrame with .loc to avoid SettingWithCopyWarning\n",
    "positive.loc[:, 'word_clean'] = positive['word'].apply(cleaningText)\n",
    "\n",
    "# Other preprocessing steps\n",
    "positive.loc[:, 'word_preprocessed'] = positive['word_clean'].apply(tokenizingText)\n",
    "positive.loc[:, 'word_preprocessed'] = positive['word_preprocessed'].apply(filteringText)\n",
    "positive.loc[:, 'word_preprocessed'] = positive['word_preprocessed'].apply(stemmingText)\n",
    "\n",
    "# Convert lists to strings to make them hashable\n",
    "positive.loc[:, 'word_preprocessed'] = positive['word_preprocessed'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Rest of your code\n",
    "positive.drop_duplicates(subset='word_preprocessed', inplace=True)\n",
    "nan_value = float(\"NaN\")\n",
    "positive.replace(\"\", nan_value, inplace=True)\n",
    "positive.dropna(subset=['word_clean'], inplace=True) \n",
    "\n",
    "# Save to CSV\n",
    "positive.to_csv(r'positive_clean.csv', encoding=\"utf-8\", index=False, header=True, index_label=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive words after preprocessing\")\n",
    "positive.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clean negative and save it to csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negative = negative.copy()\n",
    "\n",
    "# Use .loc to avoid SettingWithCopyWarning\n",
    "negative.loc[:, 'word_clean'] = negative['word'].apply(cleaningText)\n",
    "\n",
    "# Dropping the original 'word' column\n",
    "negative.drop(['word'], axis=1, inplace=True)\n",
    "\n",
    "# Preprocessing steps\n",
    "negative.loc[:, 'word_preprocessed'] = negative['word_clean'].apply(tokenizingText)\n",
    "negative.loc[:, 'word_preprocessed'] = negative['word_preprocessed'].apply(filteringText)\n",
    "negative.loc[:, 'word_preprocessed'] = negative['word_preprocessed'].apply(stemmingText)\n",
    "\n",
    "# Convert list to string to make them hashable for drop_duplicates\n",
    "negative.loc[:, 'word_preprocessed'] = negative['word_preprocessed'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Dropping duplicates\n",
    "negative.drop_duplicates(subset='word_preprocessed', inplace=True)\n",
    "\n",
    "# Replacing empty strings with NaN and dropping NaN values\n",
    "nan_value = float(\"NaN\")\n",
    "negative.replace(\"\", nan_value, inplace=True)\n",
    "negative.dropna(subset=['word_clean'], inplace=True)\n",
    "\n",
    "# Saving to CSV\n",
    "negative.to_csv(r'negative_clean.csv', encoding=\"utf-8\", index=False, header=True, index_label=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define positive word dictionary\n",
    "dict_positive = dict()\n",
    "\n",
    "# the path of positive clean words\n",
    "myfile = 'positive_clean.csv'\n",
    "positive_data = pd.read_csv(myfile, encoding='utf-8')\n",
    "\n",
    "# we used this variable to make use of the length\n",
    "positive = positive_data[['word_clean', 'polarity']]\n",
    "for i in range(len(positive)): \n",
    "    dict_positive[positive_data['word_clean'][i].strip()] = int(positive_data['polarity'][i])\n",
    "\n",
    "\n",
    "# define negative word dictionary\n",
    "dict_negative = dict()\n",
    "\n",
    "# the path of the negative clean words\n",
    "myfile = 'negative_clean.csv'\n",
    "negative_data = pd.read_csv(myfile, encoding='utf-8')\n",
    "\n",
    "# we used this variable to make use of lenght\n",
    "negative = negative_data[['word_clean', 'polarity']]\n",
    "for i in range(len(negative)):  \n",
    "    dict_negative[negative_data['word_clean'][i].strip()] = int(negative_data['polarity'][i])\n",
    "\n",
    "# the dictionary of posiitive words keys are positive words and values are polarity\n",
    "print(\"The polarity of word ممتاز:\")\n",
    "print(dict_positive['ممتاز'])\n",
    "# the dictionary of negative wors keys are negative words and values are polarity\n",
    "print(\"The polarity of word تعيس:\")\n",
    "print(dict_negative['تعيس'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# دالة حساب قطبية قائمة من الكلمات       \n",
    "def sentiment_analysis_dict_arabic(words_list):\n",
    "    score = 0\n",
    "    for word in words_list:\n",
    "        if (word in dict_positive):\n",
    "            score = score + dict_positive[word]\n",
    "    for word in words_list:\n",
    "        if (word in dict_negative):\n",
    "            score = score + dict_negative[word]\n",
    "    polarity=''\n",
    "    if (score > 0):\n",
    "        polarity = 'positive'\n",
    "    elif (score < 0):\n",
    "        polarity = 'negative'\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    "    return score, polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# حساب قطبية التغريدات \n",
    "results = tweets['tweet_preprocessed'].apply(sentiment_analysis_dict_arabic)\n",
    "results = list(zip(*results))\n",
    "tweets['polarity_score'] = results[0]\n",
    "tweets['polarity'] = results[1]\n",
    "\n",
    "# كتابة النتائج في ملف\n",
    "tweets.to_csv(r'tweets_clean_polarity.csv', encoding='utf-8', index = False, header = True,index_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# رسم نسب قطبية التغريدات\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "# حساب عدد التغريدات من كل قطبية\n",
    "x = [count for count in tweets['polarity'].value_counts()]\n",
    "# تسميات الرسم\n",
    "labels = list(tweets['polarity'].value_counts().index)\n",
    "explode = (0.1, 0, 0)\n",
    "# تنفيذ الرسم\n",
    "ax.pie(x = x, labels = labels, autopct = '%1.1f%%', explode = explode, textprops={'fontsize': 14})\n",
    "# عنوان الرسم\n",
    "ax.set_title('Tweets Polarities ', fontsize = 16, pad = 20)\n",
    "# الإظهار\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# طباعة أكثر التغريدات إيجابية\n",
    "pd.set_option('display.max_colwidth', 3000)\n",
    "positive_tweets = tweets[tweets['polarity'] == 'positive']\n",
    "positive_tweets = positive_tweets[['tweet_clean', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=False).reset_index(drop = True)\n",
    "positive_tweets.index += 1\n",
    "positive_tweets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# طباعة أكثر التغريدات سلبية\n",
    "pd.set_option('display.max_colwidth', 3000)\n",
    "negative_tweets = tweets[tweets['polarity'] == 'negative']\n",
    "negative_tweets = negative_tweets[['tweet_clean', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=True)[0:10].reset_index(drop = True)\n",
    "negative_tweets.index += 1\n",
    "negative_tweets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install python-bidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# سحابة الكلمات\n",
    "from wordcloud import WordCloud\n",
    "# مكتبة للغة العربية\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "# انتقاء بعض الكلمات المعالجة\n",
    "list_words=''\n",
    "i=0\n",
    "for tweet in tweets['tweet_preprocessed']:\n",
    "    for word in tweet:\n",
    "        i=i+1\n",
    "        if i>100:\n",
    "            break\n",
    "        list_words += ' '+(word)\n",
    "# ضبط اللغة العربية\n",
    "reshaped_text = arabic_reshaper.reshape(list_words)\n",
    "artext = get_display(reshaped_text)\n",
    "# إعدادات سحابة الكلمات\n",
    "wordcloud = WordCloud(font_path='DroidSansMono.ttf', width = 600, height = 400, background_color = 'black', min_font_size = 10).generate(artext)\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "# عنوان السحابة\n",
    "ax.set_title('Word Cloud of Tweets', fontsize = 18)\n",
    "ax.grid(False)\n",
    "ax.imshow((wordcloud))\n",
    "fig.tight_layout(pad=0)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تجميع الكلمات الموجبة والكلمات السالبة\n",
    "def words_with_sentiment(list_words):\n",
    "    positive_words=[]\n",
    "    negative_words=[]\n",
    "    for word in list_words:\n",
    "        score_pos = 0\n",
    "        score_neg = 0\n",
    "        if (word in dict_positive):\n",
    "            score_pos = dict_positive[word]\n",
    "        if (word in dict_negative):\n",
    "            score_neg = dict_negative[word]\n",
    "        \n",
    "        if (score_pos + score_neg > 0):\n",
    "            positive_words.append(word)\n",
    "        elif (score_pos + score_neg < 0):\n",
    "            negative_words.append(word)\n",
    "            \n",
    "    return positive_words, negative_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
